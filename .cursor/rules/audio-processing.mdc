---
description: 
globs: 
alwaysApply: true
---
---
description: 音频处理相关的规范和API
globs: "**/audio/**/*.{js,jsx,ts,tsx}"
alwaysApply: true
---

# 音频处理规范

本文档详细说明语言学习音频播放器系统中的音频处理规范和 API 使用指南。所有音频处理相关代码必须严格遵循这些规范。

## Web Audio API 基础

系统使用 Web Audio API 实现音频处理功能，基本音频处理流程如下：

```
           ┌───────────────┐
           │  AudioContext │
           └───────┬───────┘
                   │
                   ▼
┌──────────────────────────────────┐
│  AudioSource                      │
│  (AudioBufferSourceNode /         │
│   MediaElementSourceNode)         │
└──────────────┬───────────────────┘
               │
               ▼
┌──────────────────────────────────┐
│  Processing Nodes                 │
│  (AnalyserNode, GainNode,         │
│   BiquadFilterNode, etc.)         │
└──────────────┬───────────────────┘
               │
               ▼
┌──────────────────────────────────┐
│  AudioDestinationNode             │
└──────────────────────────────────┘
```

### 创建音频上下文

```typescript
// 创建音频上下文
function createAudioContext() {
  // 音频上下文必须在用户交互后创建
  // 避免自动播放策略限制
  const context = new AudioContext();
  return context;
}

// 自动恢复被暂停的上下文
function resumeAudioContext(context: AudioContext) {
  if (context.state === 'suspended') {
    context.resume();
  }
  return context.state === 'running';
}
```

### 音频源管理

系统支持两种类型的音频源：

1. **AudioBufferSourceNode** - 用于内存中的音频处理
2. **MediaElementSourceNode** - 用于流式音频处理

```typescript
// 从音频元素创建源节点
function createSourceFromElement(context: AudioContext, element: HTMLAudioElement) {
  const source = context.createMediaElementSource(element);
  return source;
}

// 从 ArrayBuffer 创建源节点
async function createSourceFromBuffer(context: AudioContext, arrayBuffer: ArrayBuffer) {
  // 解码音频数据
  const audioBuffer = await context.decodeAudioData(arrayBuffer);
  
  // 创建缓冲源节点
  const source = context.createBufferSource();
  source.buffer = audioBuffer;
  
  return {
    source,
    buffer: audioBuffer
  };
}
```

## 音频处理链

音频处理链定义了音频信号的处理流程，系统使用以下标准处理链：

```typescript
// 标准处理链设置
function setupProcessingChain(context: AudioContext, source: AudioNode) {
  // 创建分析器节点 - 用于波形可视化
  const analyser = context.createAnalyser();
  analyser.fftSize = 2048;
  
  // 创建增益节点 - 用于音量控制
  const gainNode = context.createGain();
  
  // 创建均衡器节点 - 用于音频均衡
  const eq = {
    bass: context.createBiquadFilter(),
    mid: context.createBiquadFilter(),
    treble: context.createBiquadFilter()
  };
  
  // 配置均衡器
  eq.bass.type = 'lowshelf';
  eq.bass.frequency.value = 200;
  
  eq.mid.type = 'peaking';
  eq.mid.frequency.value = 1000;
  eq.mid.Q.value = 1;
  
  eq.treble.type = 'highshelf';
  eq.treble.frequency.value = 3000;
  
  // 创建压缩器 - 用于动态范围控制
  const compressor = context.createDynamicsCompressor();
  
  // 连接节点
  source.connect(analyser);
  analyser.connect(eq.bass);
  eq.bass.connect(eq.mid);
  eq.mid.connect(eq.treble);
  eq.treble.connect(gainNode);
  gainNode.connect(compressor);
  compressor.connect(context.destination);
  
  return {
    analyser,
    gainNode,
    eq,
    compressor
  };
}
```

## 波形可视化

波形可视化是系统的核心功能，必须高效实现：

```typescript
// 获取波形数据
function getWaveformData(analyser: AnalyserNode) {
  const bufferLength = analyser.frequencyBinCount;
  const dataArray = new Uint8Array(bufferLength);
  analyser.getByteTimeDomainData(dataArray);
  return dataArray;
}

// 绘制波形
function drawWaveform(canvas: HTMLCanvasElement, dataArray: Uint8Array) {
  const ctx = canvas.getContext('2d');
  if (!ctx) return;
  
  const width = canvas.width;
  const height = canvas.height;
  
  // 清除画布
  ctx.clearRect(0, 0, width, height);
  
  // 设置绘制样式
  ctx.lineWidth = 2;
  ctx.strokeStyle = '#3b82f6';
  
  // 开始绘制路径
  ctx.beginPath();
  
  const sliceWidth = width / dataArray.length;
  let x = 0;
  
  for (let i = 0; i < dataArray.length; i++) {
    const v = dataArray[i] / 128.0;
    const y = v * height / 2;
    
    if (i === 0) {
      ctx.moveTo(x, y);
    } else {
      ctx.lineTo(x, y);
    }
    
    x += sliceWidth;
  }
  
  ctx.stroke();
}
```

### 高性能波形绘制

对于长音频，使用抽样和缓存策略：

```typescript
// 生成静态波形数据
async function generateStaticWaveform(audioBuffer: AudioBuffer, numPoints: number) {
  // 抽样策略 - 通过 WebAssembly 实现高性能处理
  const audioData = audioBuffer.getChannelData(0);
  
  // 如果支持 WebAssembly，使用 Rust 处理
  if (window.wasmAudioProcessor) {
    return window.wasmAudioProcessor.generateWaveform(audioData, numPoints);
  }
  
  // 回退到 JavaScript 实现
  const result = new Float32Array(numPoints);
  const sampleSize = Math.floor(audioData.length / numPoints);
  
  for (let i = 0; i < numPoints; i++) {
    const start = i * sampleSize;
    const end = start + sampleSize;
    
    let min = audioData[start];
    let max = audioData[start];
    
    for (let j = start; j < end; j++) {
      if (audioData[j] < min) min = audioData[j];
      if (audioData[j] > max) max = audioData[j];
    }
    
    result[i] = Math.max(Math.abs(min), Math.abs(max));
  }
  
  return result;
}
```

## AB循环实现

AB循环功能是语言学习的关键功能：

```typescript
// 设置 AB 循环
function setupABLoop(audioElement: HTMLAudioElement, loopStart: number, loopEnd: number) {
  // 移除现有监听器
  audioElement.removeEventListener('timeupdate', handleTimeUpdate);
  
  // 定义时间更新处理函数
  function handleTimeUpdate() {
    if (audioElement.currentTime >= loopEnd) {
      audioElement.currentTime = loopStart;
    }
  }
  
  // 添加监听器
  audioElement.addEventListener('timeupdate', handleTimeUpdate);
  
  return {
    clear: () => {
      audioElement.removeEventListener('timeupdate', handleTimeUpdate);
    }
  };
}
```

## WebAssembly 集成

系统使用 Rust + WebAssembly 实现高性能音频处理：

```typescript
// 加载 WebAssembly 处理器
async function loadWasmProcessor() {
  try {
    // 动态导入 WebAssembly 模块
    const wasmModule = await import('@/lib/audio/wasm/audio_processor.js');
    
    // 初始化模块
    await wasmModule.default();
    
    // 导出处理器实例
    const processor = wasmModule.AudioProcessor.new();
    
    // 全局注册处理器 (仅用于演示，实际应通过 Context API 共享)
    window.wasmAudioProcessor = processor;
    
    return processor;
  } catch (error) {
    console.error('Failed to load WebAssembly processor:', error);
    return null;
  }
}
```

### Rust WebAssembly 接口

```rust
// 在 Rust 中实现的接口
#[wasm_bindgen]
impl AudioProcessor {
    #[wasm_bindgen(constructor)]
    pub fn new() -> Self {
        AudioProcessor {
            // 初始化处理器
        }
    }
    
    // 生成波形数据
    #[wasm_bindgen]
    pub fn generate_waveform(&self, audio_data: &[f32], num_points: u32) -> Box<[f32]> {
        // 实现波形生成算法
        let mut result = vec![0.0; num_points as usize];
        
        // 计算每个点代表的样本数
        let samples_per_point = audio_data.len() / num_points as usize;
        
        for i in 0..num_points as usize {
            let start = i * samples_per_point;
            let end = (i + 1) * samples_per_point;
            let end = end.min(audio_data.len());
            
            // 查找这段区间的最大绝对值
            let mut max_amplitude = 0.0;
            for j in start..end {
                let amplitude = audio_data[j].abs();
                if amplitude > max_amplitude {
                    max_amplitude = amplitude;
                }
            }
            
            result[i] = max_amplitude;
        }
        
        result.into_boxed_slice()
    }
    
    // 应用均衡器
    #[wasm_bindgen]
    pub fn apply_equalizer(&self, audio_data: &mut [f32], bass: f32, mid: f32, treble: f32) {
        // 实现均衡器算法
    }
    
    // 分析音频获取特征
    #[wasm_bindgen]
    pub fn analyze_audio(&self, audio_data: &[f32]) -> JsValue {
        // 计算音频特征并返回 JS 对象
        let rms = self.calculate_rms(audio_data);
        let peak = self.calculate_peak(audio_data);
        
        let result = AudioFeatures {
            rms,
            peak,
        };
        
        JsValue::from_serde(&result).unwrap()
    }
}
```

## 音频缓冲策略

系统使用渐进式加载策略处理长音频：

```typescript
// 音频加载状态
interface AudioLoadingState {
  isLoading: boolean;
  progress: number;
  loadedRanges: Array<{ start: number; end: number }>;
  buffer?: AudioBuffer;
  error?: Error;
}

// 渐进式加载音频
function useProgressiveAudioLoading(url: string) {
  const [state, setState] = useState<AudioLoadingState>({
    isLoading: false,
    progress: 0,
    loadedRanges: []
  });
  
  const audioContext = useRef<AudioContext | null>(null);
  
  // 初始化加载
  const startLoading = useCallback(async () => {
    if (!audioContext.current) {
      audioContext.current = new AudioContext();
    }
    
    setState(prev => ({ ...prev, isLoading: true }));
    
    try {
      // 首先加载文件头以获取基本信息
      const headerBuffer = await loadAudioRange(url, 0, 1024 * 64); // 加载前64KB
      
      // 继续加载前30秒或整个文件（如果较小）
      const initialChunk = await loadAudioRange(
        url, 
        0, 
        1024 * 1024 * 5 // 加载前5MB
      );
      
      // 解码音频
      const buffer = await audioContext.current.decodeAudioData(initialChunk);
      
      setState(prev => ({
        ...prev,
        isLoading: false,
        buffer,
        progress: 0.2, // 假设加载了20%
        loadedRanges: [{ start: 0, end: 1024 * 1024 * 5 }]
      }));
      
      // 继续在后台加载剩余部分
      loadRemainingInBackground(url, 1024 * 1024 * 5);
      
    } catch (error) {
      setState(prev => ({
        ...prev,
        isLoading: false,
        error: error as Error
      }));
    }
  }, [url]);
  
  // 加载指定范围的音频
  const loadAudioRange = useCallback(async (url: string, start: number, end: number) => {
    const response = await fetch(url, {
      headers: {
        Range: `bytes=${start}-${end}`
      }
    });
    
    return await response.arrayBuffer();
  }, []);
  
  // 在后台加载剩余部分
  const loadRemainingInBackground = useCallback(async (url: string, startByte: number) => {
    // 实现后台加载逻辑
  }, []);
  
  return {
    ...state,
    startLoading
  };
}
```

## 音频处理钩子

提供标准的音频处理钩子：

```typescript
// 使用音频处理器
function useAudioProcessor() {
  const [audioContext, setAudioContext] = useState<AudioContext | null>(null);
  const [nodes, setNodes] = useState<{
    source?: AudioNode;
    analyser?: AnalyserNode;
    gainNode?: GainNode;
    eq?: Record<string, BiquadFilterNode>;
  }>({});
  
  // 初始化音频上下文
  const initialize = useCallback(() => {
    if (audioContext) return;
    
    const ctx = new AudioContext();
    setAudioContext(ctx);
    
    return ctx;
  }, [audioContext]);
  
  // 连接音频源
  const connectSource = useCallback((source: HTMLAudioElement | AudioBuffer) => {
    if (!audioContext) return;
    
    // 清理现有连接
    if (nodes.source) {
      disconnectAll();
    }
    
    let sourceNode: AudioNode;
    
    if (source instanceof HTMLAudioElement) {
      sourceNode = audioContext.createMediaElementSource(source);
    } else {
      sourceNode = audioContext.createBufferSource();
      (sourceNode as AudioBufferSourceNode).buffer = source;
    }
    
    // 创建处理节点
    const analyser = audioContext.createAnalyser();
    analyser.fftSize = 2048;
    
    const gainNode = audioContext.createGain();
    
    const eq = {
      bass: audioContext.createBiquadFilter(),
      mid: audioContext.createBiquadFilter(),
      treble: audioContext.createBiquadFilter()
    };
    
    // 配置均衡器
    eq.bass.type = 'lowshelf';
    eq.bass.frequency.value = 200;
    
    eq.mid.type = 'peaking';
    eq.mid.frequency.value = 1000;
    
    eq.treble.type = 'highshelf';
    eq.treble.frequency.value = 3000;
    
    // 连接节点
    sourceNode.connect(analyser);
    analyser.connect(eq.bass);
    eq.bass.connect(eq.mid);
    eq.mid.connect(eq.treble);
    eq.treble.connect(gainNode);
    gainNode.connect(audioContext.destination);
    
    // 如果是缓冲源，开始播放
    if (sourceNode instanceof AudioBufferSourceNode) {
      sourceNode.start();
    }
    
    // 保存节点引用
    setNodes({
      source: sourceNode,
      analyser,
      gainNode,
      eq
    });
    
  }, [audioContext, nodes, disconnectAll]);
  
  // 断开所有连接
  const disconnectAll = useCallback(() => {
    if (!nodes.source) return;
    
    try {
      nodes.source.disconnect();
      nodes.analyser?.disconnect();
      nodes.gainNode?.disconnect();
      
      if (nodes.eq) {
        nodes.eq.bass.disconnect();
        nodes.eq.mid.disconnect();
        nodes.eq.treble.disconnect();
      }
    } catch (error) {
      console.error('Error disconnecting nodes:', error);
    }
    
    setNodes({});
  }, [nodes]);
  
  // 设置音量
  const setVolume = useCallback((volume: number) => {
    if (!nodes.gainNode) return;
    nodes.gainNode.gain.value = volume;
  }, [nodes]);
  
  // 设置均衡器
  const setEqualizer = useCallback((bass: number, mid: number, treble: number) => {
    if (!nodes.eq) return;
    
    nodes.eq.bass.gain.value = bass;
    nodes.eq.mid.gain.value = mid;
    nodes.eq.treble.gain.value = treble;
  }, [nodes]);
  
  // 获取波形数据
  const getWaveformData = useCallback(() => {
    if (!nodes.analyser) return new Uint8Array(0);
    
    const bufferLength = nodes.analyser.frequencyBinCount;
    const dataArray = new Uint8Array(bufferLength);
    nodes.analyser.getByteTimeDomainData(dataArray);
    
    return dataArray;
  }, [nodes]);
  
  // 获取频谱数据
  const getFrequencyData = useCallback(() => {
    if (!nodes.analyser) return new Uint8Array(0);
    
    const bufferLength = nodes.analyser.frequencyBinCount;
    const dataArray = new Uint8Array(bufferLength);
    nodes.analyser.getByteFrequencyData(dataArray);
    
    return dataArray;
  }, [nodes]);
  
  // 清理资源
  useEffect(() => {
    return () => {
      disconnectAll();
      
      if (audioContext) {
        audioContext.close();
      }
    };
  }, [audioContext, disconnectAll]);
  
  return {
    initialize,
    connectSource,
    disconnectAll,
    setVolume,
    setEqualizer,
    getWaveformData,
    getFrequencyData,
    audioContext,
    nodes
  };
}
```

## 音频效果

系统支持以下音频效果：

1. **均衡器** - 调整低中高频
2. **压缩器** - 控制动态范围
3. **速度控制** - 调整播放速度
4. **循环区域** - 实现精确循环

```typescript
// 实现速度控制
function usePlaybackRate(audioElement: HTMLAudioElement | null) {
  const [rate, setRate] = useState(1.0);
  
  useEffect(() => {
    if (!audioElement) return;
    audioElement.playbackRate = rate;
  }, [audioElement, rate]);
  
  const setPlaybackRate = useCallback((newRate: number) => {
    if (!audioElement) return;
    
    const clampedRate = Math.max(0.5, Math.min(2.0, newRate));
    audioElement.playbackRate = clampedRate;
    setRate(clampedRate);
  }, [audioElement]);
  
  return {
    playbackRate: rate,
    setPlaybackRate
  };
}

// 实现音频压缩
function setupCompressor(context: AudioContext) {
  const compressor = context.createDynamicsCompressor();
  
  // 默认设置语音压缩
  compressor.threshold.value = -24;
  compressor.knee.value = 30;
  compressor.ratio.value = 12;
  compressor.attack.value = 0.003;
  compressor.release.value = 0.25;
  
  return compressor;
}
```

## 安全音频加载

系统使用令牌验证确保音频安全：

```typescript
// 安全加载音频
async function secureLoadAudio(trackId: string) {
  // 首先获取访问令牌
  const tokenResponse = await fetch(`/api/audio/token/${trackId}`, {
    credentials: 'include'
  });
  
  if (!tokenResponse.ok) {
    throw new Error('Failed to get audio access token');
  }
  
  const { token } = await tokenResponse.json();
  
  // 使用令牌请求音频
  const audioUrl = `/api/audio/stream/${trackId}?token=${token}`;
  
  return audioUrl;
}
```

## 性能注意事项

1. **避免内存泄漏** - 适当释放 AudioContext 和节点
2. **减少重复解码** - 缓存已解码的音频数据
3. **使用 Web Workers** - 将重计算操作移至 Worker 线程
4. **优化可视化** - 减少波形渲染频率
5. **按需加载** - 实现渐进式音频加载

```typescript
// 在 Web Worker 中处理波形生成
// waveform.worker.ts
self.onmessage = async (e) => {
  const { audioData, width, downsampleFactor } = e.data;
  
  // 波形计算逻辑
  const result = new Float32Array(width);
  const samplesPerPoint = Math.floor(audioData.length / width) * downsampleFactor;
  
  for (let i = 0; i < width; i++) {
    const start = i * samplesPerPoint;
    const end = start + samplesPerPoint;
    
    let min = audioData[start];
    let max = audioData[start];
    
    for (let j = start; j < end && j < audioData.length; j++) {
      if (audioData[j] < min) min = audioData[j];
      if (audioData[j] > max) max = audioData[j];
    }
    
    result[i] = Math.max(Math.abs(min), Math.abs(max));
  }
  
  self.postMessage({ waveform: result });
};
```

## 浏览器兼容性

系统必须在现代浏览器中工作，并实现适当的降级策略：

```typescript
// 检测音频功能支持
function detectAudioSupport() {
  const support = {
    webAudio: 'AudioContext' in window || 'webkitAudioContext' in window,
    mediaElement: 'HTMLMediaElement' in window,
    analyser: false,
    biquadFilter: false,
    wasmSupport: typeof WebAssembly === 'object'
  };
  
  if (support.webAudio) {
    const AudioCtx = window.AudioContext || (window as any).webkitAudioContext;
    const ctx = new AudioCtx();
    
    support.analyser = typeof ctx.createAnalyser === 'function';
    support.biquadFilter = typeof ctx.createBiquadFilter === 'function';
    
    // 不要忘记关闭测试上下文
    ctx.close();
  }
  
  return support;
}
```

## 错误处理策略

音频处理中的错误处理策略：

```typescript
// 音频错误处理
function handleAudioError(error: Error, context: AudioContext | null) {
  console.error('Audio processing error:', error);
  
  // 尝试恢复 AudioContext
  if (context && context.state === 'suspended') {
    context.resume().catch(e => {
      console.error('Failed to resume audio context:', e);
    });
  }
  
  // 根据错误类型采取不同行动
  if (error.name === 'NotSupportedError') {
    // 格式不支持
    return new Error('该音频格式不受支持，请尝试不同的格式');
  } else if (error.name === 'NotAllowedError') {
    // 自动播放受限
    return new Error('需要用户交互才能播放音频');
  } else {
    // 一般错误
    return new Error('音频处理时发生错误，请刷新页面重试');
  }
}
```

## 常见错误和避免的反模式

以下是在音频处理实现中应该避免的常见错误和反模式：

### 1. 混合式音频处理方法

```typescript
// 错误: 混合使用 HTML Audio 元素和 Web Audio API
function createMixedAudioPlayer() {
  // 创建 HTML Audio 元素
  const audioElement = new Audio();
  
  // 同时创建 AudioContext
  const audioContext = new AudioContext();
  
  // 将两者混合使用
  const source = audioContext.createMediaElementSource(audioElement);
  
  return {
    play: () => {
      // 有时直接操作 audio 元素
      audioElement.play();
      
      // 有时通过 Web Audio API 操作
      // 这导致重复处理和资源浪费
    }
  };
}

// 正确: 完全使用 Web Audio API
function createAudioPlayer() {
  const audioContext = new AudioContext();
  
  async function loadAndPlay(url) {
    const response = await fetch(url);
    const arrayBuffer = await response.arrayBuffer();
    const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
    
    const source = audioContext.createBufferSource();
    source.buffer = audioBuffer;
    source.connect(audioContext.destination);
    source.start();
    
    return source;
  }
  
  return { loadAndPlay };
}
```

### 2. 忽略音频长度的加载策略

```typescript
// 错误: 对所有音频使用相同的加载策略
function loadAudio(url) {
  // 无论音频大小如何，总是使用相同方法
  fetch(url)
    .then(response => response.arrayBuffer())
    .then(arrayBuffer => audioContext.decodeAudioData(arrayBuffer));
}

// 正确: 根据音频长度选择适当的加载策略
async function loadAudio(url, options = { threshold: 5 * 1024 * 1024 }) {
  // 首先检查文件大小
  const headResponse = await fetch(url, { method: 'HEAD' });
  const contentLength = parseInt(headResponse.headers.get('content-length') || '0');
  
  if (contentLength > options.threshold) {
    // 长音频使用流媒体加载
    return loadAudioStream(url);
  } else {
    // 短音频使用完整加载
    return loadCompleteAudio(url);
  }
}
```

### 3. 缺乏有效的缓冲监控

```typescript
// 错误: 仅依赖事件监听缓冲状态
function monitorBuffering(audioElement) {
  audioElement.addEventListener('waiting', () => {
    console.log('缓冲中...');
  });
  
  audioElement.addEventListener('canplay', () => {
    console.log('可以播放');
  });
}

// 正确: 使用基于 AudioContext 的监控方法
function createBufferingMonitor(audioContext, mediaSource) {
  let lastTime = 0;
  let lastPosition = 0;
  let isBuffering = false;
  
  const checkBuffering = () => {
    const currentTime = audioContext.currentTime;
    const currentPosition = mediaSource.mediaElement.currentTime;
    
    // 检测是否正在播放但位置没有改变
    if (mediaSource.mediaElement.paused === false) {
      if (currentPosition === lastPosition && currentTime > lastTime + 0.1) {
        if (!isBuffering) {
          isBuffering = true;
          console.log('缓冲中...');
        }
      } else if (isBuffering) {
        isBuffering = false;
        console.log('继续播放');
      }
    }
    
    lastTime = currentTime;
    lastPosition = currentPosition;
  };
  
  const intervalId = setInterval(checkBuffering, 100);
  
  return {
    stop: () => clearInterval(intervalId)
  };
}
```

### 4. 资源管理不当

```typescript
// 错误: 没有适当释放 AudioNode 和 AudioContext
function createPlayer() {
  let context = new AudioContext();
  let nodes = [];
  
  function playSound(buffer) {
    const source = context.createBufferSource();
    source.buffer = buffer;
    
    const gain = context.createGain();
    
    source.connect(gain);
    gain.connect(context.destination);
    
    source.start();
    
    // 没有跟踪或释放节点
    nodes.push(source, gain);
  }
  
  // 没有提供关闭或清理方法
}

// 正确: 适当管理资源生命周期
function createPlayer() {
  let context = new AudioContext();
  let activeNodes = new Set();
  
  function playSound(buffer) {
    const source = context.createBufferSource();
    source.buffer = buffer;
    
    const gain = context.createGain();
    
    source.connect(gain);
    gain.connect(context.destination);
    
    // 跟踪活动节点
    activeNodes.add(source);
    activeNodes.add(gain);
    
    // 设置完成时的清理
    source.onended = () => {
      source.disconnect();
      gain.disconnect();
      activeNodes.delete(source);
      activeNodes.delete(gain);
    };
    
    source.start();
    
    return source;
  }
  
  // 提供清理方法
  function dispose() {
    // 断开所有活动节点
    for (const node of activeNodes) {
      try {
        node.disconnect();
      } catch (e) {
        console.error('Error disconnecting node:', e);
      }
    }
    
    activeNodes.clear();
    
    // 关闭上下文
    if (context.state !== 'closed') {
      context.close();
    }
  }
  
  return {
    playSound,
    dispose
  };
}
```

### 5. 缺少错误处理和降级策略

```typescript
// 错误: 没有错误处理和降级
async function processAudio(arrayBuffer) {
  // 假设 WebAssembly 总是可用
  const wasm = await initWebAssembly();
  return wasm.processAudioData(arrayBuffer);
}

// 正确: 提供适当的错误处理和降级
async function processAudio(arrayBuffer) {
  try {
    // 尝试使用 WebAssembly
    if (typeof WebAssembly === 'object') {
      try {
        const wasm = await initWebAssembly();
        return wasm.processAudioData(arrayBuffer);
      } catch (wasmError) {
        console.warn('WebAssembly processing failed, falling back to JS implementation:', wasmError);
      }
    }
    
    // JavaScript 降级实现
    return processAudioWithJS(arrayBuffer);
  } catch (error) {
    console.error('Audio processing error:', error);
    throw new Error('无法处理音频，请尝试其他文件或刷新页面');
  }
}
```

### 6. 未参考官方文档和示例

```typescript
// 错误: 使用非标准或过时的方法
function createAudioVisualization() {
  // 未遵循标准实践
  const analyser = audioContext.createScriptProcessor(2048, 1, 1);
  // ScriptProcessorNode 已弃用，应使用 AudioWorklet
}

// 正确: 遵循 MDN 官方示例和最佳实践
function createAudioVisualization() {
  // 使用推荐的 AnalyserNode
  const analyser = audioContext.createAnalyser();
  analyser.fftSize = 2048;
  
  // 按照 MDN 示例处理频率数据
  const bufferLength = analyser.frequencyBinCount;
  const dataArray = new Uint8Array(bufferLength);
  
  function draw() {
    requestAnimationFrame(draw);
    analyser.getByteFrequencyData(dataArray);
    // 绘制可视化...
  }
  
  draw();
  
  return analyser;
}
```

## 总体最佳实践总结

1. **完全使用 Web Audio API**: 避免混合使用 HTML Audio 元素和 Web Audio API，应完全使用 Web Audio API 处理音频。

2. **遵循官方示例**: 参考 MDN Web Audio API 官方文档和示例，确保使用标准方法和最新推荐实践。

3. **使用标准方法**: 优先使用 `AudioBufferSourceNode`、`decodeAudioData` 等标准 API 方法处理音频。

4. **自适应加载策略**: 根据音频长度选择适当的加载策略，长音频使用流媒体加载，短音频使用标准加载。

5. **精确的缓冲监控**: 使用基于 AudioContext 的监控方法，提供更精确的缓冲状态检测。

6. **资源管理优化**: 实现完善的内存和连接管理，以及明确的资源释放流程。

7. **降级和错误处理**: 为 WebAssembly 不可用情况提供 JavaScript 回退实现，实现健壮的错误处理和用户友好的反馈。

8. **性能优化**: 避免不必要的处理和转换，尤其是在音频可视化和效果处理中。